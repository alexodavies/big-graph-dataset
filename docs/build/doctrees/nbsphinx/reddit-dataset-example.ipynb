{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Example Dataset\n",
    "\n",
    "## A walkthrough of the dataset code for the Big Graph Dataset project\n",
    "\n",
    "Alex Davies, University of Bristol, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll write code to:\n",
    "\n",
    "- download a large Reddit graph from an online repository\n",
    "- sample that graph to produce a dataset of smaller graphs\n",
    "- process that dataset into a Pytorch Geometric InMemoryDataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the graph\n",
    "\n",
    "First we need to download the graph, here from the  [Stanford Network Analysis Project](http://snap.stanford.edu/data/soc-RedditHyperlinks.html)\n",
    "\n",
    "We first find the links for the graph and node features (here node features are text embeddings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_url = \"https://snap.stanford.edu/data/soc-redditHyperlinks-title.tsv\"\n",
    "embedding_url = \"http://snap.stanford.edu/data/web-redditEmbeddings-subreddits.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then play with directories and download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alex/Projects/big-graph-dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import wget\n",
    "import pandas as pd\n",
    "\n",
    "# Swap into dataset directory\n",
    "print(os.getcwd())\n",
    "start_dir = os.getcwd()\n",
    "os.chdir(\"original_datasets\")\n",
    "\n",
    "# We won't actually need this part in the final function!\n",
    "if \"reddit\" not in os.listdir():\n",
    "    os.mkdir(\"reddit\")\n",
    "    \n",
    "os.chdir(\"reddit\")\n",
    "\n",
    "#  Download raw files if we don't already have them\n",
    "if \"soc-redditHyperlinks-title.tsv\" not in os.listdir():\n",
    "    graph_data = wget.download(graph_url) # Edgelist and edge features\n",
    "if \"web-redditEmbeddings-subreddits.csv\" not in os.listdir():\n",
    "    embedding_data = wget.download(embedding_url) # Node features\n",
    "\n",
    "# We know that there are 300 components in the node feature vectors\n",
    "embedding_column_names = [\"COMPONENT\", *[i for i in range(300)]]\n",
    "embeddings = pd.read_csv(\"web-redditEmbeddings-subreddits.csv\", names=embedding_column_names).transpose()\n",
    "graph_data = pd.read_csv(\"soc-redditHyperlinks-title.tsv\", sep = \"\\t\")\n",
    "\n",
    "\n",
    "# Avoids weird directory problems\n",
    "os.chdir(start_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the node embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51268</th>\n",
       "      <th>51269</th>\n",
       "      <th>51270</th>\n",
       "      <th>51271</th>\n",
       "      <th>51272</th>\n",
       "      <th>51273</th>\n",
       "      <th>51274</th>\n",
       "      <th>51275</th>\n",
       "      <th>51276</th>\n",
       "      <th>51277</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>COMPONENT</th>\n",
       "      <td>spiders</td>\n",
       "      <td>askreddit</td>\n",
       "      <td>globaloffensivetrade</td>\n",
       "      <td>fireteams</td>\n",
       "      <td>funny</td>\n",
       "      <td>the_donald</td>\n",
       "      <td>videos</td>\n",
       "      <td>news</td>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>rocketleagueexchange</td>\n",
       "      <td>...</td>\n",
       "      <td>motleyfool</td>\n",
       "      <td>govtjobsrchinindia</td>\n",
       "      <td>snoopdogg</td>\n",
       "      <td>fortean</td>\n",
       "      <td>whatcanidoforbernie</td>\n",
       "      <td>33rd</td>\n",
       "      <td>bestofvic2015</td>\n",
       "      <td>aberystwyth</td>\n",
       "      <td>mail_forwarding</td>\n",
       "      <td>cover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.158972</td>\n",
       "      <td>-0.499114</td>\n",
       "      <td>-0.023145</td>\n",
       "      <td>2.492506</td>\n",
       "      <td>-0.81937</td>\n",
       "      <td>-0.123265</td>\n",
       "      <td>0.131896</td>\n",
       "      <td>0.132825</td>\n",
       "      <td>-2.785298</td>\n",
       "      <td>0.553341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>-0.000534</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000203</td>\n",
       "      <td>-0.001563</td>\n",
       "      <td>0.009269</td>\n",
       "      <td>0.00457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.285813</td>\n",
       "      <td>0.323983</td>\n",
       "      <td>-1.199374</td>\n",
       "      <td>-2.529917</td>\n",
       "      <td>-0.865261</td>\n",
       "      <td>-0.610208</td>\n",
       "      <td>0.866419</td>\n",
       "      <td>1.505527</td>\n",
       "      <td>-0.166391</td>\n",
       "      <td>-3.283354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052268</td>\n",
       "      <td>0.042618</td>\n",
       "      <td>0.023619</td>\n",
       "      <td>0.023847</td>\n",
       "      <td>0.017816</td>\n",
       "      <td>-0.001643</td>\n",
       "      <td>0.012698</td>\n",
       "      <td>0.004733</td>\n",
       "      <td>0.024779</td>\n",
       "      <td>0.012403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.226329</td>\n",
       "      <td>-0.424809</td>\n",
       "      <td>1.661484</td>\n",
       "      <td>-0.448484</td>\n",
       "      <td>0.301753</td>\n",
       "      <td>0.361495</td>\n",
       "      <td>0.919025</td>\n",
       "      <td>0.730393</td>\n",
       "      <td>1.592624</td>\n",
       "      <td>-3.091485</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027792</td>\n",
       "      <td>-0.021329</td>\n",
       "      <td>-0.003317</td>\n",
       "      <td>-0.018297</td>\n",
       "      <td>-0.004231</td>\n",
       "      <td>-0.002896</td>\n",
       "      <td>-0.007575</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.017018</td>\n",
       "      <td>-0.000363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.183338</td>\n",
       "      <td>-0.222705</td>\n",
       "      <td>-1.025296</td>\n",
       "      <td>-3.543441</td>\n",
       "      <td>0.018787</td>\n",
       "      <td>-1.171773</td>\n",
       "      <td>-0.765584</td>\n",
       "      <td>-0.505759</td>\n",
       "      <td>-1.269829</td>\n",
       "      <td>0.877085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013468</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.005566</td>\n",
       "      <td>-0.004873</td>\n",
       "      <td>-0.010438</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.006486</td>\n",
       "      <td>-0.000982</td>\n",
       "      <td>-0.007228</td>\n",
       "      <td>-0.002496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1                     2          3         4      \\\n",
       "COMPONENT   spiders  askreddit  globaloffensivetrade  fireteams     funny   \n",
       "0          0.158972  -0.499114             -0.023145   2.492506  -0.81937   \n",
       "1          0.285813   0.323983             -1.199374  -2.529917 -0.865261   \n",
       "2          0.226329  -0.424809              1.661484  -0.448484  0.301753   \n",
       "3         -0.183338  -0.222705             -1.025296  -3.543441  0.018787   \n",
       "\n",
       "                5         6         7                8      \\\n",
       "COMPONENT  the_donald    videos      news  leagueoflegends   \n",
       "0           -0.123265  0.131896  0.132825        -2.785298   \n",
       "1           -0.610208  0.866419  1.505527        -0.166391   \n",
       "2            0.361495  0.919025  0.730393         1.592624   \n",
       "3           -1.171773 -0.765584 -0.505759        -1.269829   \n",
       "\n",
       "                          9      ...       51268               51269  \\\n",
       "COMPONENT  rocketleagueexchange  ...  motleyfool  govtjobsrchinindia   \n",
       "0                      0.553341  ...    0.004494            0.001908   \n",
       "1                     -3.283354  ...    0.052268            0.042618   \n",
       "2                     -3.091485  ...   -0.027792           -0.021329   \n",
       "3                      0.877085  ...    0.013468               0.012   \n",
       "\n",
       "               51270     51271                51272     51273          51274  \\\n",
       "COMPONENT  snoopdogg   fortean  whatcanidoforbernie      33rd  bestofvic2015   \n",
       "0          -0.000534  0.000615             0.000906 -0.000076      -0.000203   \n",
       "1           0.023619  0.023847             0.017816 -0.001643       0.012698   \n",
       "2          -0.003317 -0.018297            -0.004231 -0.002896      -0.007575   \n",
       "3           0.005566 -0.004873            -0.010438  0.000581       0.006486   \n",
       "\n",
       "                 51275            51276     51277  \n",
       "COMPONENT  aberystwyth  mail_forwarding     cover  \n",
       "0            -0.001563         0.009269   0.00457  \n",
       "1             0.004733         0.024779  0.012403  \n",
       "2            -0.000082        -0.017018 -0.000363  \n",
       "3            -0.000982        -0.007228 -0.002496  \n",
       "\n",
       "[5 rows x 51278 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOURCE_SUBREDDIT</th>\n",
       "      <th>TARGET_SUBREDDIT</th>\n",
       "      <th>POST_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>LINK_SENTIMENT</th>\n",
       "      <th>PROPERTIES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rddtgaming</td>\n",
       "      <td>rddtrust</td>\n",
       "      <td>1u4pzzs</td>\n",
       "      <td>2013-12-31 16:39:18</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0,23.0,0.76,0.0,0.44,0.12,0.12,4.0,4.0,0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xboxone</td>\n",
       "      <td>battlefield_4</td>\n",
       "      <td>1u4tmfs</td>\n",
       "      <td>2013-12-31 17:59:11</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0,88.0,0.78,0.02,0.08,0.13,0.07,16.0,16.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ps4</td>\n",
       "      <td>battlefield_4</td>\n",
       "      <td>1u4tmos</td>\n",
       "      <td>2013-12-31 17:59:40</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0,88.0,0.78,0.02,0.08,0.13,0.07,16.0,16.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fitnesscirclejerk</td>\n",
       "      <td>leangains</td>\n",
       "      <td>1u50xfs</td>\n",
       "      <td>2013-12-31 19:01:56</td>\n",
       "      <td>1</td>\n",
       "      <td>49.0,43.0,0.775510204082,0.0,0.265306122449,0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fitnesscirclejerk</td>\n",
       "      <td>lifeprotips</td>\n",
       "      <td>1u51nps</td>\n",
       "      <td>2013-12-31 21:02:28</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0,14.0,0.785714285714,0.0,0.428571428571,0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SOURCE_SUBREDDIT TARGET_SUBREDDIT  POST_ID            TIMESTAMP  \\\n",
       "0         rddtgaming         rddtrust  1u4pzzs  2013-12-31 16:39:18   \n",
       "1            xboxone    battlefield_4  1u4tmfs  2013-12-31 17:59:11   \n",
       "2                ps4    battlefield_4  1u4tmos  2013-12-31 17:59:40   \n",
       "3  fitnesscirclejerk        leangains  1u50xfs  2013-12-31 19:01:56   \n",
       "4  fitnesscirclejerk      lifeprotips  1u51nps  2013-12-31 21:02:28   \n",
       "\n",
       "   LINK_SENTIMENT                                         PROPERTIES  \n",
       "0               1  25.0,23.0,0.76,0.0,0.44,0.12,0.12,4.0,4.0,0.0,...  \n",
       "1               1  100.0,88.0,0.78,0.02,0.08,0.13,0.07,16.0,16.0,...  \n",
       "2               1  100.0,88.0,0.78,0.02,0.08,0.13,0.07,16.0,16.0,...  \n",
       "3               1  49.0,43.0,0.775510204082,0.0,0.265306122449,0....  \n",
       "4               1  14.0,14.0,0.785714285714,0.0,0.428571428571,0....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Turn the data into a graph\n",
    "\n",
    "Now we need to turn the data into a graph. \n",
    "Our edges come from graph_data (SOURCE and TARGET), including categories for each edge (LINK_SENTIMENT), as well as edge features (PROPERTIES). \n",
    "\n",
    "The first step is making a networkx.Graph object, which is a useful graph class, then we add the nodes one by one. \n",
    "We'll include the text embedding for the subreddit as a node attribute, taken from the embedding dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\t\t\t<script type=\"text/javascript\">\n",
       "\t\t\t<!--\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_script');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('script');\n",
       "\t\t\t\telement.type = 'text/javascript';\n",
       "\t\t\t\telement.innerHTML = 'function NetworKit_pageEmbed(id) { var i, j; var elements; elements = document.getElementById(id).getElementsByClassName(\"Plot\"); for (i=0; i<elements.length; i++) { elements[i].id = id + \"_Plot_\" + i; var data = elements[i].getAttribute(\"data-image\").split(\"|\"); elements[i].removeAttribute(\"data-image\"); var content = \"<div class=\\\\\"Image\\\\\" id=\\\\\"\" + elements[i].id + \"_Image\\\\\" />\"; elements[i].innerHTML = content; elements[i].setAttribute(\"data-image-index\", 0); elements[i].setAttribute(\"data-image-length\", data.length); for (j=0; j<data.length; j++) { elements[i].setAttribute(\"data-image-\" + j, data[j]); } NetworKit_plotUpdate(elements[i]); elements[i].onclick = function (e) { NetworKit_overlayShow((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"HeatCell\"); for (i=0; i<elements.length; i++) { var data = parseFloat(elements[i].getAttribute(\"data-heat\")); var color = \"#00FF00\"; if (data <= 1 && data > 0) { color = \"hsla(0, 100%, 75%, \" + (data) + \")\"; } else if (data <= 0 && data >= -1) { color = \"hsla(240, 100%, 75%, \" + (-data) + \")\"; } elements[i].style.backgroundColor = color; } elements = document.getElementById(id).getElementsByClassName(\"Details\"); for (i=0; i<elements.length; i++) { elements[i].setAttribute(\"data-title\", \"-\"); NetworKit_toggleDetails(elements[i]); elements[i].onclick = function (e) { NetworKit_toggleDetails((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"MathValue\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"nan\") { elements[i].parentNode.innerHTML = \"\" } } elements = document.getElementById(id).getElementsByClassName(\"SubCategory\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } elements = document.getElementById(id).getElementsByClassName(\"Category\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } var isFirefox = false; try { isFirefox = typeof InstallTrigger !== \"undefined\"; } catch (e) {} if (!isFirefox) { alert(\"Currently the function\\'s output is only fully supported by Firefox.\"); } } function NetworKit_plotUpdate(source) { var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(source.id + \"_Image\"); image.style.backgroundImage = \"url(\" + data + \")\"; } function NetworKit_showElement(id, show) { var element = document.getElementById(id); element.style.display = (show) ? \"block\" : \"none\"; } function NetworKit_overlayShow(source) { NetworKit_overlayUpdate(source); NetworKit_showElement(\"NetworKit_Overlay\", true); } function NetworKit_overlayUpdate(source) { document.getElementById(\"NetworKit_Overlay_Title\").innerHTML = source.title; var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(\"NetworKit_Overlay_Image\"); image.setAttribute(\"data-id\", source.id); image.style.backgroundImage = \"url(\" + data + \")\"; var link = document.getElementById(\"NetworKit_Overlay_Toolbar_Bottom_Save\"); link.href = data; link.download = source.title + \".svg\"; } function NetworKit_overlayImageShift(delta) { var image = document.getElementById(\"NetworKit_Overlay_Image\"); var source = document.getElementById(image.getAttribute(\"data-id\")); var index = parseInt(source.getAttribute(\"data-image-index\")); var length = parseInt(source.getAttribute(\"data-image-length\")); var index = (index+delta) % length; if (index < 0) { index = length + index; } source.setAttribute(\"data-image-index\", index); NetworKit_overlayUpdate(source); } function NetworKit_toggleDetails(source) { var childs = source.children; var show = false; if (source.getAttribute(\"data-title\") == \"-\") { source.setAttribute(\"data-title\", \"+\"); show = false; } else { source.setAttribute(\"data-title\", \"-\"); show = true; } for (i=0; i<childs.length; i++) { if (show) { childs[i].style.display = \"block\"; } else { childs[i].style.display = \"none\"; } } }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_script');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_style');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('style');\n",
       "\t\t\t\telement.type = 'text/css';\n",
       "\t\t\t\telement.innerHTML = '.NetworKit_Page { font-family: Arial, Helvetica, sans-serif; font-size: 14px; } .NetworKit_Page .Value:before { font-family: Arial, Helvetica, sans-serif; font-size: 1.05em; content: attr(data-title) \":\"; margin-left: -2.5em; padding-right: 0.5em; } .NetworKit_Page .Details .Value:before { display: block; } .NetworKit_Page .Value { font-family: monospace; white-space: pre; padding-left: 2.5em; white-space: -moz-pre-wrap !important; white-space: -pre-wrap; white-space: -o-pre-wrap; white-space: pre-wrap; word-wrap: break-word; tab-size: 4; -moz-tab-size: 4; } .NetworKit_Page .Category { clear: both; padding-left: 1em; margin-bottom: 1.5em; } .NetworKit_Page .Category:before { content: attr(data-title); font-size: 1.75em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory { margin-bottom: 1.5em; padding-left: 1em; } .NetworKit_Page .SubCategory:before { font-size: 1.6em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory[data-title]:before { content: attr(data-title); } .NetworKit_Page .Block { display: block; } .NetworKit_Page .Block:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .Block .Thumbnail_Overview, .NetworKit_Page .Block .Thumbnail_ScatterPlot { width: 260px; float: left; } .NetworKit_Page .Block .Thumbnail_Overview img, .NetworKit_Page .Block .Thumbnail_ScatterPlot img { width: 260px; } .NetworKit_Page .Block .Thumbnail_Overview:before, .NetworKit_Page .Block .Thumbnail_ScatterPlot:before { display: block; text-align: center; font-weight: bold; } .NetworKit_Page .Block .Thumbnail_Overview:before { content: attr(data-title); } .NetworKit_Page .HeatCell { font-family: \"Courier New\", Courier, monospace; cursor: pointer; } .NetworKit_Page .HeatCell, .NetworKit_Page .HeatCellName { display: inline; padding: 0.1em; margin-right: 2px; background-color: #FFFFFF } .NetworKit_Page .HeatCellName { margin-left: 0.25em; } .NetworKit_Page .HeatCell:before { content: attr(data-heat); display: inline-block; color: #000000; width: 4em; text-align: center; } .NetworKit_Page .Measure { clear: both; } .NetworKit_Page .Measure .Details { cursor: pointer; } .NetworKit_Page .Measure .Details:before { content: \"[\" attr(data-title) \"]\"; display: block; } .NetworKit_Page .Measure .Details .Value { border-left: 1px dotted black; margin-left: 0.4em; padding-left: 3.5em; pointer-events: none; } .NetworKit_Page .Measure .Details .Spacer:before { content: \".\"; opacity: 0.0; pointer-events: none; } .NetworKit_Page .Measure .Plot { width: 440px; height: 440px; cursor: pointer; float: left; margin-left: -0.9em; margin-right: 20px; } .NetworKit_Page .Measure .Plot .Image { background-repeat: no-repeat; background-position: center center; background-size: contain; height: 100%; pointer-events: none; } .NetworKit_Page .Measure .Stat { width: 500px; float: left; } .NetworKit_Page .Measure .Stat .Group { padding-left: 1.25em; margin-bottom: 0.75em; } .NetworKit_Page .Measure .Stat .Group .Title { font-size: 1.1em; display: block; margin-bottom: 0.3em; margin-left: -0.75em; border-right-style: dotted; border-right-width: 1px; border-bottom-style: dotted; border-bottom-width: 1px; background-color: #D0D0D0; padding-left: 0.2em; } .NetworKit_Page .Measure .Stat .Group .List { -webkit-column-count: 3; -moz-column-count: 3; column-count: 3; } .NetworKit_Page .Measure .Stat .Group .List .Entry { position: relative; line-height: 1.75em; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:before { position: absolute; left: 0; top: -40px; background-color: #808080; color: #ffffff; height: 30px; line-height: 30px; border-radius: 5px; padding: 0 15px; content: attr(data-tooltip); white-space: nowrap; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:after { position: absolute; left: 15px; top: -10px; border-top: 7px solid #808080; border-left: 7px solid transparent; border-right: 7px solid transparent; content: \"\"; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:after, .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:before { display: block; } .NetworKit_Page .Measure .Stat .Group .List .Entry .MathValue { font-family: \"Courier New\", Courier, monospace; } .NetworKit_Page .Measure:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .PartitionPie { clear: both; } .NetworKit_Page .PartitionPie img { width: 600px; } #NetworKit_Overlay { left: 0px; top: 0px; display: none; position: absolute; width: 100%; height: 100%; background-color: rgba(0,0,0,0.6); z-index: 1000; } #NetworKit_Overlay_Title { position: absolute; color: white; transform: rotate(-90deg); width: 32em; height: 32em; padding-right: 0.5em; padding-top: 0.5em; text-align: right; font-size: 40px; } #NetworKit_Overlay .button { background: white; cursor: pointer; } #NetworKit_Overlay .button:before { size: 13px; display: inline-block; text-align: center; margin-top: 0.5em; margin-bottom: 0.5em; width: 1.5em; height: 1.5em; } #NetworKit_Overlay .icon-close:before { content: \"X\"; } #NetworKit_Overlay .icon-previous:before { content: \"P\"; } #NetworKit_Overlay .icon-next:before { content: \"N\"; } #NetworKit_Overlay .icon-save:before { content: \"S\"; } #NetworKit_Overlay_Toolbar_Top, #NetworKit_Overlay_Toolbar_Bottom { position: absolute; width: 40px; right: 13px; text-align: right; z-index: 1100; } #NetworKit_Overlay_Toolbar_Top { top: 0.5em; } #NetworKit_Overlay_Toolbar_Bottom { Bottom: 0.5em; } #NetworKit_Overlay_ImageContainer { position: absolute; top: 5%; left: 5%; height: 90%; width: 90%; background-repeat: no-repeat; background-position: center center; background-size: contain; } #NetworKit_Overlay_Image { height: 100%; width: 100%; background-repeat: no-repeat; background-position: center center; background-size: contain; }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_style');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_Overlay');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('div');\n",
       "\t\t\t\telement.innerHTML = '<div id=\"NetworKit_Overlay_Toolbar_Top\"><div class=\"button icon-close\" id=\"NetworKit_Overlay_Close\" /></div><div id=\"NetworKit_Overlay_Title\" /> <div id=\"NetworKit_Overlay_ImageContainer\"> <div id=\"NetworKit_Overlay_Image\" /> </div> <div id=\"NetworKit_Overlay_Toolbar_Bottom\"> <div class=\"button icon-previous\" onclick=\"NetworKit_overlayImageShift(-1)\" /> <div class=\"button icon-next\" onclick=\"NetworKit_overlayImageShift(1)\" /> <a id=\"NetworKit_Overlay_Toolbar_Bottom_Save\"><div class=\"button icon-save\" /></a> </div>';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_Overlay');\n",
       "\t\t\t\tdocument.body.appendChild(element);\n",
       "\t\t\t\tdocument.getElementById('NetworKit_Overlay_Close').onclick = function (e) {\n",
       "\t\t\t\t\tdocument.getElementById('NetworKit_Overlay').style.display = 'none';\n",
       "\t\t\t\t}\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t-->\n",
       "\t\t\t</script>\n",
       "\t\t"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding nodes: 100%|██████████| 51278/51278 [00:01<00:00, 32961.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# networkx is a Python library for graph structures\n",
    "import networkx as nx\n",
    "# tqdm for loading bars\n",
    "from tqdm import tqdm\n",
    "# cheeky function to visualise a networkx graph\n",
    "from utils import vis_networkx\n",
    "\n",
    "embeddings.columns = embeddings.iloc[0]\n",
    "embeddings = embeddings.drop([\"COMPONENT\"], axis = 0)\n",
    "\n",
    "graph = nx.Graph()\n",
    "\n",
    "# Add a node for each id in the embedding data\n",
    "for col in tqdm(embeddings.columns, desc = \"Adding nodes\"):\n",
    "    # attrs here is taken from the embedding data, with the node id the column (col)\n",
    "    graph.add_node(col, attrs=embeddings[col].to_numpy().astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the edges (the actual graph stuff).\n",
    "\n",
    "We need to include two properties:\n",
    "\n",
    "- Type of edge, negative or positive interaction, -1 or 1. In dataframe as LINK_SENTIMENT\n",
    "- Properties of the edge, text embedding of the reddit post content \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wrangling edge features: 100%|██████████| 571927/571927 [00:05<00:00, 103705.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fix_property_string(input_string):\n",
    "    input_string = input_string.split(',')\n",
    "    input_string = [float(item) for item in input_string]\n",
    "\n",
    "    return np.array(input_string)\n",
    "\n",
    "sources = graph_data[\"SOURCE_SUBREDDIT\"].to_numpy()\n",
    "targets = graph_data[\"TARGET_SUBREDDIT\"].to_numpy()\n",
    "\n",
    "# This line can take a while!\n",
    "attrs = [fix_property_string(properties) for properties in tqdm(graph_data[\"PROPERTIES\"].tolist(), desc = \"Wrangling edge features\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now iterate over the edges, only adding them if their nodes have data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding edges: 100%|██████████| 571927/571927 [00:37<00:00, 15434.45it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = graph_data[\"LINK_SENTIMENT\"].to_numpy()\n",
    "all_nodes = list(graph.nodes())\n",
    "\n",
    "for i in tqdm(range(sources.shape[0]), desc = \"Adding edges\"):\n",
    "    # Check that the edge has data\n",
    "    if sources[i] in all_nodes and targets[i] in all_nodes:\n",
    "        graph.add_edge(sources[i], targets[i],\n",
    "                    labels = labels[i],\n",
    "                    attr = attrs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 51278 nodes and 178143 edges\n"
     ]
    }
   ],
   "source": [
    "print(graph)\n",
    "\n",
    "# Last tidying bits\n",
    "graph = nx.convert_node_labels_to_integers(graph)\n",
    "CGs = [graph.subgraph(c) for c in nx.connected_components(graph)]\n",
    "CGs = sorted(CGs, key=lambda x: x.number_of_nodes(), reverse=True)\n",
    "graph = CGs[0]\n",
    "graph = nx.convert_node_labels_to_integers(graph)\n",
    "\n",
    "# Save the graph!\n",
    "with open(\"original_datasets/reddit/reddit-graph.npz\", \"wb\") as f:\n",
    "    pickle.dump(graph, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Graph achieved. Spot that in the last section we had to deal with some missing data - we're including edges ONLY if their nodes also have data.\n",
    "\n",
    "---\n",
    "\n",
    "## Sample to make a dataset of smaller graphs\n",
    "\n",
    "This is not too hard, I've written some code (Exploration Sampling With Replacement, ESWR) that does the sampling for you.\n",
    "\n",
    "This will produce a big list of small networkx graphs sampled from that original graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling 1000 in 5 chunks with size 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling from large graph: 100%|██████████| 6/6 [00:07<00:00,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done sampling!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import ESWR\n",
    "\n",
    "# Sample 1000 graphs of max 96 nodes from the big reddit graph\n",
    "nx_graph_list = ESWR(graph, 1000, 96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to convert it to a Pytorch Geometric format.\n",
    "This will be specific to your data - here we have node labels, edge labels, edge attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data in torch geometric format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/adgcl/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[54, 300], edge_index=[2, 249], edge_attr=[249, 86], y=[249, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def specific_from_networkx(graph):\n",
    "    # Turns a graph into a pytorch geometric object\n",
    "    # Mostly by unpacking dictionaries on nodes and edges\n",
    "    # Here node labels are the target\n",
    "    # One of these functions for each dataset ideally - they are unlikely to transfer across datasets\n",
    "    node_attrs = []\n",
    "    edge_indices = []\n",
    "    edge_labels = []\n",
    "    edge_attrs = []\n",
    "\n",
    "    # Collect node labels and attributes\n",
    "    for n in list(graph.nodes(data=True)):\n",
    "        # list(graph.nodes(data=True)) returns [(node_id1, {attribute dictionary}), (node_id2, ...), (node_id3, ...)]\n",
    "        node_attrs.append(torch.Tensor(n[1][\"attrs\"]))\n",
    "\n",
    "    # Collect edge indices and attributes\n",
    "    for e in graph.edges(data=True):\n",
    "        # graph.edges(data=True) is a generator producing (node_id1, node_id2, {attribute dictionary})\n",
    "        edge_indices.append((e[0], e[1]))\n",
    "\n",
    "        edge_attrs.append(torch.Tensor(e[2][\"attr\"])) \n",
    "        edge_labels.append(e[2][\"labels\"])\n",
    "\n",
    "\n",
    "    # Specific to classification on edges! This is a binary edge classification (pos/neg) task\n",
    "    edge_labels = ((torch.Tensor(edge_labels) + 1)/2).reshape(-1,1)\n",
    "\n",
    "    edge_attrs = torch.stack(edge_attrs)\n",
    "    node_attrs = torch.stack(node_attrs)\n",
    "    edge_indices = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Create PyG Data object\n",
    "    # Can pass:\n",
    "    # x:            node features, shape (n nodes x n features)\n",
    "    # edge_index:   the list of edges in the graph, shape (2, n_edges). Entries edge_index[i, :] are [node_id1, node_id2].\n",
    "    # edge_attr:    edge features, shape (n_edges, n_features), same order as edgelist\n",
    "    # y:            targets. Graph regression shape (n_variables), graph classification (n_classes), node classification (n_nodes, n_classes), edge classification (n_edges, n_classes)\n",
    "    data = Data(x=node_attrs, edge_index=edge_indices, edge_attr = edge_attrs,  y=edge_labels)\n",
    "\n",
    "    return data\n",
    "\n",
    "print(\"Data in torch geometric format:\")\n",
    "specific_from_networkx(nx_graph_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The final dataset\n",
    "\n",
    "Finally we place all that data into an InMemoryDataset!\n",
    "\n",
    "Please note that in-code this whole notebook will be in functions - see `datasets/example_dataset.py`.\n",
    "\n",
    "This means that the `datalist` argument wouldn't actually exist below - instead you'd call something like `get_reddit_dataset()` within your `.process` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting data to PyG data objects: 100%|██████████| 1000/1000 [00:01<00:00, 968.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cora files exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import InMemoryDataset\n",
    "from utils import vis_grid\n",
    "\n",
    "class RedditDataset(InMemoryDataset):\n",
    "    r\"\"\"\n",
    "    Reddit hyperlink graphs - ie graphs of subreddits interacting with one another.\n",
    "    The original graph is sourced from:\n",
    "\n",
    "    `Kumar, Srijan, et al. \"Community interaction and conflict on the web.\" Proceedings of the 2018 world wide web conference. 2018.`\n",
    "\n",
    "    The data has text embeddings as node features for each subreddit and text features for the cross-post edges.\n",
    "\n",
    "    The task is edge classification for the sentiment of the interaction between subreddits.\n",
    "\n",
    "    - Task: Edge classification\n",
    "    - Num node features: 300\n",
    "    - Num edge features: 86\n",
    "    - Num target values: 1\n",
    "    - Target shape: N Edges\n",
    "    - Num graphs: Parameterised by `num`\n",
    "\n",
    "    Args:\n",
    "    root (str): Root directory where the dataset should be saved.\n",
    "    datalist (list): A list of pytorch geometric data objects. Only obtained from an argument here, not in actual code!\n",
    "    stage (str): The stage of the dataset to load. One of \"train\", \"val\", \"test\". (default: :obj:`\"train\"`)\n",
    "    transform (callable, optional): A function/transform that takes in an :obj:`torch_geometric.data.Data` object and returns a transformed version. The data object will be transformed before every access. (default: :obj:`None`)\n",
    "    pre_transform (callable, optional): A function/transform that takes in an :obj:`torch_geometric.data.Data` object and returns a transformed version. The data object will be transformed before being saved to disk. (default: :obj:`None`)\n",
    "    pre_filter (callable, optional): A function that takes in an :obj:`torch_geometric.data.Data` object and returns a boolean value, indicating whether the data object should be included in the final dataset. (default: :obj:`None`)\n",
    "    num (int): The number of samples to take from the original dataset. (default: :obj:`2000`).\n",
    "    \"\"\"\n",
    "\n",
    "    # datalist is only an argument for this notebook example - see existing datasets under datasets/.. for how it actually works\n",
    "    def __init__(self, root, datalist, stage = \"train\", transform=None, pre_transform=None, pre_filter=None, num = 2000):\n",
    "        self.num = num\n",
    "        self.stage = stage\n",
    "        self.stage_to_index = {\"train\":0,\n",
    "                               \"val\":1,\n",
    "                               \"test\":2}\n",
    "        self.datalist = datalist\n",
    "\n",
    "        # Options are node-classification, node-regression, graph-classification, graph-regression, edge-regression, edge-classification\n",
    "        # Graph-level tasks are preferred! (graph-classification and graph-regression)\n",
    "        # edge-prediction is another option if you can't think of a good task\n",
    "        self.task = \"node-classification\"\n",
    "\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[self.stage_to_index[self.stage]])\n",
    "\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        # Replace with your saved raw file name\n",
    "        return ['reddit-graph.npz']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['train.pt',\n",
    "                'val.pt',\n",
    "                'test.pt']\n",
    "\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "\n",
    "        if os.path.isfile(self.processed_paths[self.stage_to_index[self.stage]]):\n",
    "            print(f\"Cora files exist\")\n",
    "            return\n",
    "\n",
    "        # Get a list of num pytorch_geometric.data.Data objects\n",
    "        data_list = self.datalist # get_example_dataset(num=self.num)\n",
    "\n",
    "        # Torch geometric stuff\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        # Save data\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[self.stage_to_index[self.stage]])\n",
    "\n",
    "pyg_graphs = [specific_from_networkx(g) for g in tqdm(nx_graph_list, desc = \"Converting data to PyG data objects\")]\n",
    "\n",
    "# The visualisation doesn't work in documentation :( uncomment to see the graphs\n",
    "# vis_grid(pyg_graphs[:16], os.getcwd() + \"/original_datasets/reddit/example.png\", show_plot = True)\n",
    "\n",
    "train_dataset = RedditDataset(os.getcwd() + \"/original_datasets/reddit\",\n",
    "                              pyg_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other datsets\n",
    "\n",
    "There are already some datasets we can look at under datasets/DATASET.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general-gcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
